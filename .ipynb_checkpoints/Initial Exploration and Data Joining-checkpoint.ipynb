{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79bbd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29331d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./raw_data_2010_2023/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dabd2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = [ 'SLC','CWA','MEM', 'BTV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d07608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_files(folder_path, suffix=\".csv\"):\n",
    "    \"\"\"\n",
    "    Find all CSV files in the given folder path.\n",
    "\n",
    "    Args:\n",
    "    - folder_path (str): Path to the folder where CSV files are located.\n",
    "    - suffix (str): Suffix to filter files (default is '.csv').\n",
    "\n",
    "    Returns:\n",
    "    - list: List of CSV files with the given suffix.\n",
    "    \"\"\"\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(suffix):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56dbfdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_n_categorical(data, column, n=None, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Plot the distribution of the top N categories of a categorical variable.\n",
    "\n",
    "    Args:\n",
    "    - data (pandas DataFrame): DataFrame containing the categorical variable.\n",
    "    - column (str): Name of the categorical variable column.\n",
    "    - n (int or None): Number of top categories to plot. If None, plot all categories (default is None).\n",
    "    - figsize (tuple): Width and height of the figure in inches (default is (10, 6)).\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot).\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        categories = data[column].value_counts().index\n",
    "    else:\n",
    "        categories = data[column].value_counts().nlargest(n).index\n",
    "        \n",
    "    data_filtered = data[data[column].isin(categories)]\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.countplot(data=data_filtered, x=column, order=categories)\n",
    "    plt.title(f'Top {len(categories)} Categories of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a207f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_files = find_csv_files(DATA_DIR, suffix=\"_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e59d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for f in all_data_files:\n",
    "    all_dfs.append(pd.read_csv(f, parse_dates=[\"Date (MM/DD/YYYY)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "971bbf08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_merged \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mldl/lib/python3.9/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/envs/mldl/lib/python3.9/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mldl/lib/python3.9/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "all_merged = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24d2aab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_merged\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "all_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd9b179a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sorted_all_merged \u001b[38;5;241m=\u001b[39m \u001b[43mall_merged\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate (MM/DD/YYYY)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScheduled Arrival Time\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_all_merged = all_merged.sort_values(by=['Date (MM/DD/YYYY)', 'Scheduled Arrival Time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf0f99d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sorted_all_merged \u001b[38;5;241m=\u001b[39m \u001b[43msorted_all_merged\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_all_merged = sorted_all_merged.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59174638",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msorted_all_merged\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_all_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "008e1b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sorted_all_merged \u001b[38;5;241m=\u001b[39m \u001b[43msorted_all_merged\u001b[49m[\u001b[38;5;241m~\u001b[39msorted_all_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrigin Airport\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(drop)]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_all_merged = sorted_all_merged[~sorted_all_merged['Origin Airport'].isin(drop)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d542c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msorted_all_merged\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_all_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "204ff16b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sorted_all_merged \u001b[38;5;241m=\u001b[39m \u001b[43msorted_all_merged\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate (MM/DD/YYYY)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScheduled Arrival Time\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_all_merged = sorted_all_merged.sort_values(by=['Date (MM/DD/YYYY)', 'Scheduled Arrival Time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6de7b0",
   "metadata": {},
   "source": [
    "# CARRIER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "723330ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_top_n_categorical(\u001b[43msorted_all_merged\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCarrier Code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "plot_top_n_categorical(sorted_all_merged, \"Carrier Code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0108b8",
   "metadata": {},
   "source": [
    "# FLIGHT NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cda25546",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_top_n_categorical(\u001b[43msorted_all_merged\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlight Number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m40\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "plot_top_n_categorical(sorted_all_merged, \"Flight Number\", 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c4650",
   "metadata": {},
   "source": [
    "# TAIL NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a40041c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_top_n_categorical(\u001b[43msorted_all_merged\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTail Number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "plot_top_n_categorical(sorted_all_merged, 'Tail Number', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a2796",
   "metadata": {},
   "source": [
    "# ORIGIN AIRPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "919860ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_top_n_categorical(\u001b[43msorted_all_merged\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrigin Airport\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "plot_top_n_categorical(sorted_all_merged, 'Origin Airport')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63144260",
   "metadata": {},
   "source": [
    "# COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5f3ca53",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msorted_all_merged\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_all_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e40f7ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Wheels-on Time', 'Taxi-In time (Minutes)',\n",
    "       'Delay Carrier (Minutes)', 'Delay Weather (Minutes)',\n",
    "       'Delay National Aviation System (Minutes)', 'Delay Security (Minutes)',\n",
    "       'Delay Late Aircraft Arrival (Minutes)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10e37c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_season(date):\n",
    "    \"\"\"\n",
    "    Determine the season for a given date without considering the year.\n",
    "\n",
    "    Parameters:\n",
    "    date (Timestamp): The date.\n",
    "\n",
    "    Returns:\n",
    "    str: The season corresponding to the given date.\n",
    "    \"\"\"\n",
    "    # Define fixed date ranges for each season\n",
    "    seasons = {\n",
    "        'winter': ((1, 1), (3, 20)),\n",
    "        'spring': ((3, 21), (6, 20)),\n",
    "        'summer': ((6, 21), (9, 22)),\n",
    "        'autumn': ((9, 23), (12, 20)),\n",
    "        'winter': ((12, 21), (12, 31)),\n",
    "        'winter': ((1, 1), (1, 31))\n",
    "    }\n",
    "\n",
    "    # Get month and day from the date\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    print(date.year, date.month, date.day)\n",
    "    # Determine the season based on the month and day\n",
    "    for season, (start_md, end_md) in seasons.items():\n",
    "        if start_md <= (month, day) <= end_md:\n",
    "            return season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7af8e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_categorical_column(df):\n",
    "    \"\"\"\n",
    "    Create a categorical column in the DataFrame based on the range of a numeric column by iterating over each row.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame.\n",
    "    numeric_column (str): The name of the numeric column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with the new categorical column.\n",
    "    \"\"\"\n",
    "    numeric_column = 'Arrival Delay (Minutes)'\n",
    "    categorical_values = []\n",
    "    for index, row in df.iterrows():\n",
    "        numeric_value = row[numeric_column]\n",
    "        # Customize the conditions based on your requirement\n",
    "        if abs(numeric_value) <= 5:\n",
    "            categorical_values.append('ONTIME')\n",
    "        elif numeric_value < -5:\n",
    "            categorical_values.append('EARLY')\n",
    "        elif numeric_value > 5:\n",
    "            categorical_values.append('LATE')\n",
    "        else:\n",
    "            print(\"THIS SHOULDN't HAPPEN\")\n",
    "    df['FLIGHT_STATUS'] = categorical_values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5de2f57e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[43msorted_all_merged\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mto_drop)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "processed = sorted_all_merged.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fe2041f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed \u001b[38;5;241m=\u001b[39m create_categorical_column(\u001b[43mprocessed\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "processed = create_categorical_column(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9466ae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[43mprocessed\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Elapsed Time (Minutes)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Arrival Time\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "processed = processed.drop(columns=[ 'Actual Elapsed Time (Minutes)', 'Actual Arrival Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c444bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_to_season(month):\n",
    "    \"\"\"\n",
    "    Determine the season for a given month.\n",
    "\n",
    "    Parameters:\n",
    "    month (int): The month number (1-12).\n",
    "\n",
    "    Returns:\n",
    "    str: The season corresponding to the given month.\n",
    "    \"\"\"\n",
    "    # Define mapping of months to seasons\n",
    "    season_map = {\n",
    "        1: 'winter',\n",
    "        2: 'winter',\n",
    "        3: 'spring',\n",
    "        4: 'spring',\n",
    "        5: 'spring',\n",
    "        6: 'summer',\n",
    "        7: 'summer',\n",
    "        8: 'summer',\n",
    "        9: 'autumn',\n",
    "        10: 'autumn',\n",
    "        11: 'autumn',\n",
    "        12: 'winter'\n",
    "    }\n",
    "    \n",
    "    return season_map.get(month, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1e48509",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mprocessed\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate (MM/DD/YYYY)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "processed['month'] = processed['Date (MM/DD/YYYY)'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfd5ba48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mprocessed\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate (MM/DD/YYYY)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mday\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "processed['day'] = processed['Date (MM/DD/YYYY)'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a3352fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mprocessed\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(month_to_season)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "processed['season'] = processed['month'].apply(month_to_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34b33f73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeekDay\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mprocessed\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate (MM/DD/YYYY)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mday_name()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "processed['WeekDay'] = processed['Date (MM/DD/YYYY)'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "910c5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed.to_csv(\"./INITIAL_PROCESSED_DATA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aca5833a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocessed\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f9366",
   "metadata": {},
   "source": [
    "# DATA PREP FOR 2nd Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a6ebc57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocessed\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate (MM/DD/YYYY)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScheduled Arrival Time\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "processed.sort_values(by=['Date (MM/DD/YYYY)', 'Scheduled Arrival Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "600a1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_2nd_model_data(df, prev_status='ONTIME'):\n",
    "    prevStatus = []\n",
    "    for index, row in df.iterrows():\n",
    "        prevStatus.append(prev_status)\n",
    "        prev_status = row['FLIGHT_STATUS']\n",
    "    df['PREV_STAT'] = pd.Series(prevStatus)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dfa5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_2nd_model_data_ext(df, num_prev=3):\n",
    "    extended_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        for i in range(num_prev):\n",
    "            if index - i - 1 >= 0:\n",
    "                extended_row = row.copy()  # Create a copy of the current row\n",
    "                extended_row['PREV_STAT'] = df.iloc[index - i - 1]['FLIGHT_STATUS']\n",
    "                extended_data.append(extended_row)\n",
    "            else:\n",
    "                extended_row = row.copy()  # Create a copy of the current row\n",
    "                extended_row['PREV_STAT'] = 'ONTIME'\n",
    "                extended_data.append(extended_row)\n",
    "    df_extended = pd.DataFrame(extended_data)\n",
    "    return df_extended.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae36e039",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocessed\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArrival Delay (Minutes)\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "final_data = processed.drop(columns=['Arrival Delay (Minutes)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4794418d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinal_data\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_data' is not defined"
     ]
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10e802e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_data_2nd_model_data(final_data).to_csv(\"1_HOP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e31f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_data_2nd_model_data_ext(final_data).to_csv(\"3_HOP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e46bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84be96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9282d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_to_csv_mapping = {\n",
    "    'ORD': '72530094846.csv', \n",
    "    'JFK': '74486094789.csv', \n",
    "    'ATL': '72219013874.csv', \n",
    "    'CLT': '72314013881.csv', \n",
    "    'DTW': '72537094847.csv', \n",
    "    'MCO': '72205012815.csv', \n",
    "    'LGA': '72503014732.csv', \n",
    "    'EWR': '72502014734.csv', \n",
    "    'IAD': '72403093738.csv', \n",
    "    'SYR':'72519014771.csv',\n",
    "    'DCA': '72405013743.csv', \n",
    "    'BWI': '72406093721.csv', \n",
    "    'PHL': '72408013739.csv', \n",
    "    'MSP': '72658014922.csv', \n",
    "    'BOS': '72509014739.csv', \n",
    "    'DEN': '72565003017.csv', \n",
    "    'FLL': '74783012849.csv', \n",
    "    'DFW': '72259303985.csv', \n",
    "    'CLE': '72524014820.csv', \n",
    "    'PIE': '72211612873.csv', \n",
    "    'BNA': '72327013897.csv', \n",
    "    'SFB': '72205712854.csv', \n",
    "    'RDU': '72306013722.csv', \n",
    "    'SRQ': '72211512871.csv', \n",
    "    'CVG': '72421093814.csv', \n",
    "    'RSW': '72210812894.csv',\n",
    "    'TPA': '72211012842.csv', \n",
    "    'PGD': '72203412812.csv', \n",
    "    'MYR': '74791013717.csv', \n",
    "    'MIA': '72202012839.csv',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79f4ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import urllib.request\n",
    "\n",
    "# WEATHER_REPO = './weather/'\n",
    "# years = list(range(2010, 2023))\n",
    "# BASE_LINK = 'https://www.ncei.noaa.gov/data/global-hourly/access/'\n",
    "# airports = airport_to_csv_mapping.keys()\n",
    "\n",
    "# for ap in airports:\n",
    "#     apdir = os.path.join(WEATHER_REPO, ap)  # Create directory path for each airport\n",
    "#     os.makedirs(apdir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "#     for y in years:\n",
    "#         file_url = BASE_LINK + str(y) + \"/\" + airport_to_csv_mapping[ap]\n",
    "#         file_name = os.path.basename(file_url)  # Extract filename from URL\n",
    "#         # Add year to the filename\n",
    "#         file_name_with_year = str(y) + \"_\" + file_name\n",
    "#         file_path = os.path.join(apdir, file_name_with_year)  # Create full file path\n",
    "#         urllib.request.urlretrieve(file_url, file_path)  # Download file into correct directory\n",
    "#         print(\"Downloaded:\", file_name_with_year, \"into\", apdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb21d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_consider = [\"STATION\",\"DATE\",\"SOURCE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\n",
    "                       \"REPORT_TYPE\",\"CALL_SIGN\",\"QUALITY_CONTROL\",\"WND\",\"CIG\",\"VIS\",\"TMP\",\"DEW\",\"SLP\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b242c49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import urllib.request\n",
    "\n",
    "# WEATHER_REPO = './weather/'\n",
    "# years = list(range(2009, 2025))\n",
    "# BASE_LINK = 'https://www.ncei.noaa.gov/data/global-hourly/access/'\n",
    "# airports = airport_to_csv_mapping.keys()\n",
    "\n",
    "# # Specify columns you want to consider\n",
    "# columns_to_consider = [\"STATION\",\"DATE\",\"SOURCE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\n",
    "#                        \"REPORT_TYPE\",\"CALL_SIGN\",\"QUALITY_CONTROL\",\"WND\",\"CIG\",\"VIS\",\"TMP\",\"DEW\",\"SLP\"]\n",
    "\n",
    "\n",
    "# # Dictionary to store DataFrame for each airport\n",
    "# airport_dfs = {}\n",
    "\n",
    "# for ap in airports:\n",
    "#     # List to store DataFrames for each year\n",
    "#     dfs_per_year = []\n",
    "    \n",
    "#     for y in years:\n",
    "#         try:\n",
    "#             file_url = BASE_LINK + str(y) + \"/\" + airport_to_csv_mapping[ap]\n",
    "#             # Read only specific columns from CSV data directly from URL into DataFrame\n",
    "#             df = pd.read_csv(file_url, usecols=columns_to_consider)\n",
    "#             # Append DataFrame to list\n",
    "#             dfs_per_year.append(df)\n",
    "#             print(ap, file_url)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             print(ap, file_url)\n",
    "\n",
    "#     # Concatenate DataFrames for each year into a single DataFrame\n",
    "#     airport_df = pd.concat(dfs_per_year, ignore_index=True)\n",
    "#     # Store DataFrame in dictionary\n",
    "#     airport_dfs[ap] = airport_df\n",
    "\n",
    "#     # Save the concatenated DataFrame to a CSV file in the weather repository\n",
    "#     file_name = os.path.join(WEATHER_REPO, f\"{ap}_weather_data.csv\")\n",
    "#     airport_df.to_csv(file_name, index=False)\n",
    "\n",
    "# # Access DataFrame for a specific airport\n",
    "# # For example, to access DataFrame for airport 'JFK':\n",
    "# # jfk_df = airport_dfs['JFK']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7cf01508",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import urllib.request\n",
    "\n",
    "# WEATHER_REPO = './weather/'\n",
    "# WEATHER_WORKING_REPOS = './weather_working/'\n",
    "# years = list(range(2010, 2024))\n",
    "# # BASE_LINK = 'https://www.ncei.noaa.gov/data/global-hourly/access/'\n",
    "# airports = airport_to_csv_mapping.keys()\n",
    "\n",
    "# # Specify columns you want to consider\n",
    "# columns_to_consider = [\"STATION\",\"DATE\",\"SOURCE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\n",
    "#                        \"REPORT_TYPE\",\"CALL_SIGN\",\"QUALITY_CONTROL\",\"WND\",\"CIG\",\"VIS\",\"TMP\",\"DEW\",\"SLP\"]\n",
    "\n",
    "\n",
    "# # Dictionary to store DataFrame for each airport\n",
    "# airport_dfs = {}\n",
    "\n",
    "# for ap in airports:\n",
    "#     file_url = WEATHER_REPO+ f\"{ap}_weather_data.csv\"\n",
    "#     df = pd.read_csv(file_url, parse_dates=[\"DATE\"])\n",
    "#     df['STATION'] = ap\n",
    "#     df.to_csv(WEATHER_WORKING_REPOS+ f\"{ap}_weather_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfb96958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_WND(wnd):\n",
    "    split = wnd.split(',')\n",
    "    data = {\n",
    "        'wind_direction':int(split[0].strip()) if int(split[0].strip()) != 999 else np.nan,\n",
    "        'wind_directon_quality':split[1].strip(),\n",
    "        'wind_type':split[2].strip(),\n",
    "        'wind_speed':int(split[3].strip()) if int(split[3].strip())!=9999 else np.nan,\n",
    "        'wind_speed_quality':split[4].strip()\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def parse_CIG(cig):\n",
    "    split = cig.split(',')\n",
    "    data = {\n",
    "        'ceiling_height':int(split[0].strip()) if int(split[0].strip())!=99999 else np.nan ,\n",
    "        'ceiling_height_quality':split[1].strip(),\n",
    "        'ceiling_det_code':split[2].strip(),\n",
    "        'celing_CAVOK':split[3].strip()\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def parse_VIS(vis):\n",
    "    split = vis.split(',')\n",
    "    data = {\n",
    "        'visibility_dist':int(split[0].strip()) if int(split[0].strip()) != 999999 else np.nan,\n",
    "        'visibility_dist_quality':split[1].strip(),\n",
    "        'visibility_variability':split[2].strip(),\n",
    "        'visibility_variability_quality':split[3].strip(),\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def parse_TMP(tmp):\n",
    "    split = tmp.split(',')\n",
    "    data = {\n",
    "        'air_temparature':int(split[0].strip()) if int(split[0].strip())!=9999 else np.nan,\n",
    "        'air_temparature_quality':split[1].strip()\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def parse_DEW(dew):\n",
    "    split = dew.split(',')\n",
    "    data = {\n",
    "        'dew_point_temparature':int(split[0].strip()) if int(split[0].strip())!=9999 else np.nan,\n",
    "        'dew_point_temparature_quality':split[1].strip()\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def parse_SLP(slp):\n",
    "    split = slp.split(',')\n",
    "    data = {\n",
    "        'sea_level_pressure':int(split[0].strip()) if int(split[0].strip())!=99999 else np.nan,\n",
    "        'sea_level_pressure_quality':split[1].strip()\n",
    "    }\n",
    "    return data\n",
    "\n",
    "## CREATING PARSED DATA FRAME FOR WEATHER DATA\n",
    "def parse_data(weather_data):\n",
    "    rows = []\n",
    "    cols = weather_data.columns\n",
    "    for index, row in weather_data.iterrows():\n",
    "        new_row = {}\n",
    "        for c in cols:\n",
    "            if c == 'WND':\n",
    "                parsed = parse_WND(row[c])\n",
    "                new_row.update(parsed)\n",
    "            elif c == 'CIG':\n",
    "                parsed = parse_CIG(row[c])\n",
    "                new_row.update(parsed)\n",
    "            elif c == 'VIS':\n",
    "                parsed = parse_VIS(row[c])\n",
    "                new_row.update(parsed)\n",
    "            elif c == 'TMP':\n",
    "                parsed = parse_TMP(row[c])\n",
    "                new_row.update(parsed)\n",
    "            elif c == 'DEW':\n",
    "                parsed = parse_DEW(row[c])\n",
    "                new_row.update(parsed)\n",
    "            elif c == 'SLP':\n",
    "                parsed = parse_SLP(row[c])\n",
    "                new_row.update(parsed)\n",
    "            else:\n",
    "                new_row[c.lower()] = row[c]\n",
    "        rows.append(new_row)\n",
    "    \n",
    "    parsed_df = pd.DataFrame(rows, index=weather_data.index)\n",
    "    return parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f390997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_raw_weather_data = find_csv_files('./weather_working/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98fe9b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_raw_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63ef7dba",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_raw_weather_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "all_raw_weather_data[0].split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef4984",
   "metadata": {},
   "source": [
    "### PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b90456c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = \"./PARSED_WEATHER_DATA/\"\n",
    "# os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "# for f in all_raw_weather_data:\n",
    "#     name = f.split('/')[-1]\n",
    "#     data = pd.read_csv(f)\n",
    "#     data_processed = parse_data(data)\n",
    "#     data_processed.to_csv(SAVE_PATH+name, index=False)\n",
    "#     print(f\"PROCESSED .. {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d585c03",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5fab22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv = find_csv_files(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9427f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def interpolate_target(df, columns):\n",
    "    \"\"\"\n",
    "    Interpolates missing values in the target column by averaging the five non-null values before and after the missing row.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame with missing values.\n",
    "        target_column (str): The name of the target column with missing values.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with missing values in the target column interpolated.\n",
    "    \"\"\"\n",
    "    df_interpolated = df.copy()\n",
    "    for i, row in df_interpolated.iterrows():\n",
    "        for target_column in columns:\n",
    "            if pd.isnull(row[target_column]):\n",
    "                start_idx = max(0, i - 10)\n",
    "                end_idx = min(len(df_interpolated), i + 10)\n",
    "                valid_values = df_interpolated.iloc[start_idx:end_idx][target_column].dropna()\n",
    "                if len(valid_values) > 0:\n",
    "                    avg_value = valid_values.mean()\n",
    "                    df_interpolated.at[i, target_column] = avg_value\n",
    "    return df_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82218147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING ... ./PARSED_WEATHER_DATA/MSP_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11,24,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/MSP_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/ORD_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/ORD_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/CLE_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/CLE_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/PIE_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/PIE_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/RDU_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/RDU_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/EWR_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/EWR_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/MCO_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/MCO_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/PHL_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11,14,26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/PHL_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/CVG_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/CVG_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/DEN_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11,14,20,22,24,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/DEN_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/DCA_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/DCA_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/DFW_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/DFW_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/LGA_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/LGA_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/RSW_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/RSW_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/BNA_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/BNA_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/ATL_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/ATL_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/IAD_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/IAD_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/SRQ_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/SRQ_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/CLT_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (2,11,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/CLT_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/PGD_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/PGD_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/SYR_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/SYR_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/BOS_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (2,11,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/BOS_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/MIA_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/MIA_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/FLL_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/FLL_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/MYR_weather_data.csv\n",
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/MYR_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/DTW_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11,14,20,22,24,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/DTW_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/SFB_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/SFB_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/TPA_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/TPA_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/JFK_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11,14,24,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/JFK_weather_data.csv\n",
      "PROCESSING ... ./PARSED_WEATHER_DATA/BWI_weather_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178843/3401700835.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  single = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DONE ====> ./PARSED_WEATHER_DATA/BWI_weather_data.csv\n"
     ]
    }
   ],
   "source": [
    "IMPUTED_PATH = './imputed_weather/'\n",
    "os.makedirs(IMPUTED_PATH, exist_ok=True)\n",
    "for f in all_csv:\n",
    "    print(f\"PROCESSING ... {f}\")\n",
    "    name = f.split('/')[-1]\n",
    "    single = pd.read_csv(f)\n",
    "    droplist = [\n",
    "         'source',\n",
    "        'name', 'report_type', 'call_sign', 'quality_control',\n",
    "        'wind_directon_quality',\n",
    "         'wind_speed_quality',\n",
    "        'ceiling_height_quality',\n",
    "        'visibility_dist_quality',\n",
    "        'air_temparature_quality',\n",
    "        'dew_point_temparature_quality',\n",
    "        'sea_level_pressure_quality',\n",
    "        'visibility_variability_quality'\n",
    "    ]\n",
    "    na_cols = single.columns[single.isna().any()].tolist()\n",
    "    single = single.drop(columns=droplist)\n",
    "    imputed = interpolate_target(single, na_cols)\n",
    "#     assert imputed.isna().sum().sum() == 0\n",
    "    imputed.to_csv(IMPUTED_PATH+name, index=False)\n",
    "    print(f\"PROCESSING DONE ====> {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401d5db",
   "metadata": {},
   "source": [
    "#### final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7f3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688300b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d49dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b8764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6ff1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36b154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
